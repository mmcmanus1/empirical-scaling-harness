{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empirical Scaling Harness: SwiGLU vs GeLU\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YOUR_USERNAME/empirical-scaling-harness/blob/main/scaling_experiment.ipynb)\n",
    "\n",
    "This notebook runs a complete scaling law experiment comparing SwiGLU and GeLU activations.\n",
    "\n",
    "**Hypothesis**: Does SwiGLU shift the scaling exponent α, or merely provide a constant offset in compute efficiency compared to GeLU?\n",
    "\n",
    "## Runtime Estimates\n",
    "- **Anchors only** (3M, 10M, 30M): ~2-3 hours on Colab GPU\n",
    "- **Full sweep** (includes 85M holdout): ~6-8 hours on Colab GPU\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch transformers datasets tensorboard scipy matplotlib pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - EDIT THESE\n",
    "RUN_HOLDOUT = True  # Set to False for faster run (anchors only)\n",
    "ACTIVATIONS = [\"gelu\", \"swiglu\"]  # Which activations to test\n",
    "\n",
    "# Model sizes\n",
    "ANCHOR_PARAMS = [3_000_000, 10_000_000, 30_000_000]\n",
    "HOLDOUT_PARAMS = [85_000_000]\n",
    "ALL_PARAMS = ANCHOR_PARAMS + (HOLDOUT_PARAMS if RUN_HOLDOUT else [])\n",
    "\n",
    "print(f\"Will train {len(ACTIVATIONS)} activations × {len(ALL_PARAMS)} sizes = {len(ACTIVATIONS) * len(ALL_PARAMS)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from dataclasses import dataclass\nfrom typing import Literal\nimport math\n\n@dataclass\nclass ModelConfig:\n    \"\"\"Configuration for a decoder-only transformer.\"\"\"\n    n_layers: int\n    d_model: int\n    n_heads: int\n    d_ff: int\n    vocab_size: int\n    max_seq_len: int\n    activation: Literal[\"gelu\", \"swiglu\"]\n    dropout: float = 0.0\n\n    @property\n    def head_dim(self) -> int:\n        return self.d_model // self.n_heads\n\n    def count_parameters(self) -> int:\n        embed_params = self.vocab_size * self.d_model\n        attn_params = 4 * self.d_model * self.d_model\n        if self.activation == \"swiglu\":\n            ffn_params = 3 * self.d_model * self.d_ff\n        else:\n            ffn_params = 2 * self.d_model * self.d_ff\n        ln_params = 4 * self.d_model * self.n_layers + 2 * self.d_model\n        layer_params = (attn_params + ffn_params) * self.n_layers\n        return embed_params + layer_params + ln_params\n\nMODEL_SCALES = {\n    \"3M\": {\"d_model\": 256, \"n_layers\": 4},\n    \"10M\": {\"d_model\": 384, \"n_layers\": 6},\n    \"30M\": {\"d_model\": 512, \"n_layers\": 8},\n    \"85M\": {\"d_model\": 768, \"n_layers\": 12},\n}\n\ndef compute_ffn_dim(d_model: int, activation: str) -> int:\n    if activation == \"swiglu\":\n        d_ff = int(8 * d_model / 3)\n        d_ff = (d_ff // 64) * 64\n        if d_ff == 0:\n            d_ff = 64\n    else:\n        d_ff = 4 * d_model\n    return d_ff\n\ndef build_config(target_params: int, activation: str = \"gelu\", vocab_size: int = 50257, max_seq_len: int = 512) -> ModelConfig:\n    scale_name = None\n    for name, scale in MODEL_SCALES.items():\n        scale_params = int(name.replace(\"M\", \"\")) * 1_000_000\n        if scale_params >= target_params * 0.8 and scale_params <= target_params * 1.2:\n            scale_name = name\n            break\n\n    if scale_name:\n        base_config = MODEL_SCALES[scale_name]\n        d_model = base_config[\"d_model\"]\n        n_layers = base_config[\"n_layers\"]\n    else:\n        d_model = int(math.sqrt(target_params / 20))\n        d_model = max(64, (d_model // 64) * 64)\n        n_layers = max(2, d_model // 64)\n\n    d_ff = compute_ffn_dim(d_model, activation)\n    n_heads = max(1, d_model // 64)\n\n    config = ModelConfig(\n        n_layers=n_layers, d_model=d_model, n_heads=n_heads, d_ff=d_ff,\n        vocab_size=vocab_size, max_seq_len=max_seq_len, activation=activation,\n    )\n\n    actual_params = config.count_parameters()\n    while actual_params < target_params * 0.9 and n_layers < 24:\n        n_layers += 1\n        config = ModelConfig(n_layers=n_layers, d_model=d_model, n_heads=n_heads, d_ff=d_ff,\n                            vocab_size=vocab_size, max_seq_len=max_seq_len, activation=activation)\n        actual_params = config.count_parameters()\n\n    while actual_params > target_params * 1.1 and n_layers > 1:\n        n_layers -= 1\n        config = ModelConfig(n_layers=n_layers, d_model=d_model, n_heads=n_heads, d_ff=d_ff,\n                            vocab_size=vocab_size, max_seq_len=max_seq_len, activation=activation)\n        actual_params = config.count_parameters()\n\n    return config\n\ndef get_training_config(params: int) -> dict:\n    if params <= 5_000_000:\n        return {\"batch_size\": 32, \"gradient_accumulation_steps\": 1, \"learning_rate\": 3e-4, \"use_amp\": False}\n    elif params <= 15_000_000:\n        return {\"batch_size\": 16, \"gradient_accumulation_steps\": 2, \"learning_rate\": 2e-4, \"use_amp\": False}\n    elif params <= 50_000_000:\n        return {\"batch_size\": 8, \"gradient_accumulation_steps\": 4, \"learning_rate\": 1e-4, \"use_amp\": True}\n    else:\n        return {\"batch_size\": 4, \"gradient_accumulation_steps\": 8, \"learning_rate\": 6e-5, \"use_amp\": True}\n\ndef compute_optimal_tokens(params: int) -> int:\n    return 20 * params\n\ndef compute_flops(params: int, tokens: int) -> float:\n    return 6 * params * tokens\n\n# Test config generation\nprint(\"Model Configurations:\")\nfor params in ALL_PARAMS:\n    for act in [\"gelu\", \"swiglu\"]:\n        cfg = build_config(params, act)\n        print(f\"  {act} {params/1e6:.0f}M -> {cfg.count_parameters()/1e6:.2f}M actual (L={cfg.n_layers}, d={cfg.d_model})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.w_gate = nn.Linear(d_model, d_ff, bias=False)\n",
    "        self.w_up = nn.Linear(d_model, d_ff, bias=False)\n",
    "        self.w_down = nn.Linear(d_ff, d_model, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.w_down(F.silu(self.w_gate(x)) * self.w_up(x)))\n",
    "\n",
    "class GeLUFFN(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.w_up = nn.Linear(d_model, d_ff)\n",
    "        self.w_down = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.w_down(F.gelu(self.w_up(x))))\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.n_heads = config.n_heads\n",
    "        self.head_dim = config.d_model // config.n_heads\n",
    "        self.d_model = config.d_model\n",
    "        self.qkv = nn.Linear(config.d_model, 3 * config.d_model, bias=False)\n",
    "        self.out_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.register_buffer(\"causal_mask\",\n",
    "            torch.tril(torch.ones(config.max_seq_len, config.max_seq_len)).view(1, 1, config.max_seq_len, config.max_seq_len))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = qkv.split(self.d_model, dim=-1)\n",
    "        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        scale = 1.0 / math.sqrt(self.head_dim)\n",
    "        attn = (q @ k.transpose(-2, -1)) * scale\n",
    "        attn = attn.masked_fill(self.causal_mask[:, :, :T, :T] == 0, float('-inf'))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = (attn @ v).transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.out_proj(out)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.d_model)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln2 = nn.LayerNorm(config.d_model)\n",
    "        self.ffn = SwiGLU(config.d_model, config.d_ff, config.dropout) if config.activation == \"swiglu\" else GeLUFFN(config.d_model, config.d_ff, config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.token_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.pos_emb = nn.Embedding(config.max_seq_len, config.d_model)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(config) for _ in range(config.n_layers)])\n",
    "        self.ln_f = nn.LayerNorm(config.d_model)\n",
    "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.token_emb.weight\n",
    "        self.apply(self._init_weights)\n",
    "        for block in self.blocks:\n",
    "            nn.init.normal_(block.attn.out_proj.weight, std=0.02 / math.sqrt(2 * config.n_layers))\n",
    "            if hasattr(block.ffn, 'w_down'):\n",
    "                nn.init.normal_(block.ffn.w_down.weight, std=0.02 / math.sqrt(2 * config.n_layers))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            nn.init.ones_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, input_ids, labels=None):\n",
    "        B, T = input_ids.shape\n",
    "        pos = torch.arange(0, T, device=input_ids.device).unsqueeze(0)\n",
    "        x = self.token_emb(input_ids) + self.pos_emb(pos)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.cross_entropy(logits[:, :-1, :].contiguous().view(-1, self.config.vocab_size),\n",
    "                                   labels[:, 1:].contiguous().view(-1), ignore_index=-100)\n",
    "        return logits, loss\n",
    "\n",
    "    def count_parameters(self) -> int:\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Model architecture defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "class TinyStoriesDataset(Dataset):\n",
    "    def __init__(self, split=\"train\", max_seq_len=512, max_samples=None):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        print(f\"Loading TinyStories {split}...\")\n",
    "        self.dataset = load_dataset(\"roneneldan/TinyStories\", split=split)\n",
    "        if max_samples:\n",
    "            self.dataset = self.dataset.select(range(min(max_samples, len(self.dataset))))\n",
    "        print(f\"Loaded {len(self.dataset)} samples\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokenizer(self.dataset[idx][\"text\"], max_length=self.max_seq_len,\n",
    "                               truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        input_ids = tokens[\"input_ids\"].squeeze(0)\n",
    "        return {\"input_ids\": input_ids, \"labels\": input_ids.clone()}\n",
    "\n",
    "def create_dataloaders(batch_size=32, max_seq_len=512, max_train_samples=None, max_val_samples=10000):\n",
    "    train_ds = TinyStoriesDataset(\"train\", max_seq_len, max_train_samples)\n",
    "    val_ds = TinyStoriesDataset(\"validation\", max_seq_len, max_val_samples)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "print(\"Data loading functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import time\nimport json\nimport csv\nfrom pathlib import Path\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torch.amp import autocast, GradScaler\nfrom tqdm.auto import tqdm\n\ndef train_model(model, config, train_loader, val_loader, total_tokens, run_name,\n                batch_size=32, grad_accum=1, lr=3e-4, use_amp=False, device=\"cuda\"):\n    model = model.to(device)\n    \n    # Optimizer\n    decay_params = [p for n, p in model.named_parameters() if p.requires_grad and \"emb\" not in n and \"ln\" not in n and \"bias\" not in n]\n    no_decay_params = [p for n, p in model.named_parameters() if p.requires_grad and (\"emb\" in n or \"ln\" in n or \"bias\" in n)]\n    optimizer = AdamW([{\"params\": decay_params, \"weight_decay\": 0.1}, {\"params\": no_decay_params, \"weight_decay\": 0.0}], lr=lr, betas=(0.9, 0.95))\n    \n    tokens_per_step = batch_size * grad_accum * config.max_seq_len\n    total_steps = total_tokens // tokens_per_step\n    scheduler = CosineAnnealingLR(optimizer, T_max=total_steps, eta_min=lr * 0.1)\n    scaler = GradScaler('cuda') if (use_amp and device == \"cuda\") else None\n    \n    # Logging\n    output_dir = Path(f\"logs/{run_name}\")\n    output_dir.mkdir(parents=True, exist_ok=True)\n    log_data = []\n    \n    print(f\"\\nTraining {run_name}: {total_steps} steps, {total_tokens:,} tokens\")\n    \n    model.train()\n    train_iter = iter(train_loader)\n    tokens_processed = 0\n    best_val_loss = float(\"inf\")\n    start_time = time.time()\n    \n    pbar = tqdm(range(total_steps), desc=run_name)\n    for step in pbar:\n        acc_loss = 0.0\n        for _ in range(grad_accum):\n            try:\n                batch = next(train_iter)\n            except StopIteration:\n                train_iter = iter(train_loader)\n                batch = next(train_iter)\n            \n            input_ids = batch[\"input_ids\"].to(device)\n            labels = batch[\"labels\"].to(device)\n            tokens_processed += input_ids.numel()\n            \n            with autocast('cuda', enabled=use_amp):\n                _, loss = model(input_ids, labels=labels)\n                loss = loss / grad_accum\n            \n            if scaler:\n                scaler.scale(loss).backward()\n            else:\n                loss.backward()\n            acc_loss += loss.item() * grad_accum\n        \n        if scaler:\n            scaler.unscale_(optimizer)\n        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        \n        if scaler:\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            optimizer.step()\n        optimizer.zero_grad(set_to_none=True)\n        \n        # Warmup\n        if step < 100:\n            for pg in optimizer.param_groups:\n                pg[\"lr\"] = lr * (step + 1) / 100\n        else:\n            scheduler.step()\n        \n        # Eval every 500 steps\n        if (step + 1) % 500 == 0 or step == total_steps - 1:\n            model.eval()\n            val_loss = 0.0\n            n_batches = 0\n            with torch.no_grad():\n                for vb in val_loader:\n                    vi = vb[\"input_ids\"].to(device)\n                    vl = vb[\"labels\"].to(device)\n                    with autocast('cuda', enabled=use_amp):\n                        _, vloss = model(vi, labels=vl)\n                    val_loss += vloss.item()\n                    n_batches += 1\n                    if n_batches >= 50:\n                        break\n            val_loss /= n_batches\n            best_val_loss = min(best_val_loss, val_loss)\n            model.train()\n            pbar.set_postfix({\"loss\": f\"{acc_loss:.3f}\", \"val\": f\"{val_loss:.3f}\"})\n            log_data.append({\"step\": step + 1, \"train_loss\": acc_loss, \"val_loss\": val_loss, \"tokens\": tokens_processed})\n    \n    elapsed = time.time() - start_time\n    params = model.count_parameters()\n    flops = compute_flops(params, tokens_processed)\n    \n    results = {\n        \"run_name\": run_name,\n        \"params\": params,\n        \"tokens\": tokens_processed,\n        \"flops\": flops,\n        \"final_val_loss\": val_loss,\n        \"best_val_loss\": best_val_loss,\n        \"training_time\": elapsed,\n        \"activation\": config.activation,\n    }\n    \n    with open(output_dir / \"results.json\", \"w\") as f:\n        json.dump(results, f, indent=2)\n    with open(output_dir / \"log.json\", \"w\") as f:\n        json.dump(log_data, f, indent=2)\n    \n    print(f\"  Done! Val loss: {val_loss:.4f}, Time: {elapsed/60:.1f}min\")\n    return results\n\nprint(\"Training function defined.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all results\n",
    "all_results = {}\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Running on: {device}\")\n",
    "print(f\"Total experiments: {len(ACTIVATIONS) * len(ALL_PARAMS)}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all experiments\n",
    "for activation in ACTIVATIONS:\n",
    "    for target_params in ALL_PARAMS:\n",
    "        run_name = f\"{activation}_{target_params // 1_000_000}M\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"EXPERIMENT: {run_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Build config\n",
    "        config = build_config(target_params, activation=activation)\n",
    "        actual_params = config.count_parameters()\n",
    "        train_hparams = get_training_config(actual_params)\n",
    "        total_tokens = compute_optimal_tokens(actual_params)\n",
    "        \n",
    "        print(f\"Params: {actual_params:,} | Tokens: {total_tokens:,}\")\n",
    "        print(f\"Batch: {train_hparams['batch_size']} | Grad accum: {train_hparams['gradient_accumulation_steps']} | AMP: {train_hparams['use_amp']}\")\n",
    "        \n",
    "        # Build model\n",
    "        model = Transformer(config)\n",
    "        \n",
    "        # Load data\n",
    "        train_loader, val_loader = create_dataloaders(\n",
    "            batch_size=train_hparams[\"batch_size\"],\n",
    "            max_seq_len=config.max_seq_len,\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        results = train_model(\n",
    "            model=model,\n",
    "            config=config,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            total_tokens=total_tokens,\n",
    "            run_name=run_name,\n",
    "            batch_size=train_hparams[\"batch_size\"],\n",
    "            grad_accum=train_hparams[\"gradient_accumulation_steps\"],\n",
    "            lr=train_hparams[\"learning_rate\"],\n",
    "            use_amp=train_hparams[\"use_amp\"],\n",
    "            device=device,\n",
    "        )\n",
    "        \n",
    "        all_results[run_name] = results\n",
    "        \n",
    "        # Free memory\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Save all results\n",
    "with open(\"logs/all_results.json\", \"w\") as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL EXPERIMENTS COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analysis: Fit Power Laws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# Convert results to DataFrame\n",
    "df = pd.DataFrame([\n",
    "    {\n",
    "        \"run_name\": name,\n",
    "        \"activation\": data[\"activation\"],\n",
    "        \"params\": data[\"params\"],\n",
    "        \"tokens\": data[\"tokens\"],\n",
    "        \"flops\": data[\"flops\"],\n",
    "        \"val_loss\": data[\"final_val_loss\"],\n",
    "    }\n",
    "    for name, data in all_results.items()\n",
    "])\n",
    "\n",
    "# Mark holdout\n",
    "df[\"is_holdout\"] = df[\"params\"] > 50_000_000\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_power_law(flops, losses):\n",
    "    \"\"\"Fit L(C) = a * C^(-b) in log space.\"\"\"\n",
    "    log_C = np.log(flops)\n",
    "    log_L = np.log(losses)\n",
    "    coeffs = np.polyfit(log_C, log_L, 1)\n",
    "    b = -coeffs[0]\n",
    "    a = np.exp(coeffs[1])\n",
    "    predicted = coeffs[0] * log_C + coeffs[1]\n",
    "    ss_res = np.sum((log_L - predicted) ** 2)\n",
    "    ss_tot = np.sum((log_L - np.mean(log_L)) ** 2)\n",
    "    r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0\n",
    "    return a, b, r2\n",
    "\n",
    "def power_law(C, a, b):\n",
    "    return a * np.power(C, -b)\n",
    "\n",
    "# Fit for each activation (using anchors only)\n",
    "fits = {}\n",
    "for activation in [\"gelu\", \"swiglu\"]:\n",
    "    anchor_data = df[(df[\"activation\"] == activation) & (~df[\"is_holdout\"])]\n",
    "    if len(anchor_data) >= 2:\n",
    "        a, b, r2 = fit_power_law(anchor_data[\"flops\"].values, anchor_data[\"val_loss\"].values)\n",
    "        fits[activation] = {\"a\": a, \"b\": b, \"r2\": r2}\n",
    "        print(f\"{activation.upper()}: L(C) = {a:.4e} × C^(-{b:.4f})  [R² = {r2:.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate on holdout\n",
    "print(\"\\nHoldout Validation:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for activation in [\"gelu\", \"swiglu\"]:\n",
    "    if activation not in fits:\n",
    "        continue\n",
    "    holdout = df[(df[\"activation\"] == activation) & (df[\"is_holdout\"])]\n",
    "    if len(holdout) > 0:\n",
    "        actual = holdout[\"val_loss\"].values[0]\n",
    "        predicted = power_law(holdout[\"flops\"].values[0], fits[activation][\"a\"], fits[activation][\"b\"])\n",
    "        error = abs(predicted - actual) / actual * 100\n",
    "        print(f\"{activation.upper()}: Predicted {predicted:.4f} | Actual {actual:.4f} | Error {error:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Scaling Plot (The Money Shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "colors = {\"gelu\": \"#1f77b4\", \"swiglu\": \"#d62728\"}\n",
    "C_range = np.logspace(14, 18, 100)\n",
    "\n",
    "for activation in [\"gelu\", \"swiglu\"]:\n",
    "    if activation not in fits:\n",
    "        continue\n",
    "    a, b = fits[activation][\"a\"], fits[activation][\"b\"]\n",
    "    \n",
    "    # Fitted curve\n",
    "    plt.loglog(C_range, power_law(C_range, a, b), \"-\", color=colors[activation],\n",
    "               label=f\"{activation.upper()}: L = {a:.2e} × C^(-{b:.3f})\", linewidth=2, alpha=0.7)\n",
    "    \n",
    "    # Anchor points\n",
    "    anchors = df[(df[\"activation\"] == activation) & (~df[\"is_holdout\"])]\n",
    "    plt.loglog(anchors[\"flops\"], anchors[\"val_loss\"], \"o\", color=colors[activation],\n",
    "               markersize=10, markeredgecolor=\"white\", markeredgewidth=2)\n",
    "    \n",
    "    # Holdout\n",
    "    holdout = df[(df[\"activation\"] == activation) & (df[\"is_holdout\"])]\n",
    "    if len(holdout) > 0:\n",
    "        plt.loglog(holdout[\"flops\"], holdout[\"val_loss\"], \"X\", color=colors[activation],\n",
    "                   markersize=15, markeredgewidth=3, label=f\"{activation.upper()} holdout (85M)\")\n",
    "\n",
    "plt.xlabel(\"Compute (FLOPs)\", fontsize=14)\n",
    "plt.ylabel(\"Validation Loss\", fontsize=14)\n",
    "plt.title(\"Scaling Laws: SwiGLU vs GeLU Activation\", fontsize=16, fontweight=\"bold\")\n",
    "plt.legend(loc=\"upper right\", fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add findings box\n",
    "if \"gelu\" in fits and \"swiglu\" in fits:\n",
    "    text = f\"Key Findings:\\n• GeLU exponent: {fits['gelu']['b']:.4f}\\n• SwiGLU exponent: {fits['swiglu']['b']:.4f}\\n• Difference: {abs(fits['swiglu']['b'] - fits['gelu']['b']):.4f}\"\n",
    "    plt.text(0.02, 0.02, text, transform=plt.gca().transAxes, fontsize=11,\n",
    "             verticalalignment=\"bottom\", bbox=dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"scaling_plot.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.savefig(\"scaling_plot.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPlot saved to scaling_plot.png and scaling_plot.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"CONCLUSION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if \"gelu\" in fits and \"swiglu\" in fits:\n",
    "    gelu_b = fits[\"gelu\"][\"b\"]\n",
    "    swiglu_b = fits[\"swiglu\"][\"b\"]\n",
    "    exp_diff = abs(swiglu_b - gelu_b) / gelu_b * 100\n",
    "    \n",
    "    coeff_ratio = fits[\"swiglu\"][\"a\"] / fits[\"gelu\"][\"a\"]\n",
    "    coeff_change = (1 - coeff_ratio) * 100 if coeff_ratio < 1 else -(coeff_ratio - 1) * 100\n",
    "    \n",
    "    print(f\"\"\"\n",
    "Hypothesis: Does SwiGLU shift the scaling exponent, or merely provide\n",
    "a constant offset in compute efficiency compared to GeLU?\n",
    "\n",
    "RESULTS:\n",
    "• GeLU scaling exponent (b):   {gelu_b:.4f}\n",
    "• SwiGLU scaling exponent (b): {swiglu_b:.4f}\n",
    "• Exponent difference:         {exp_diff:.1f}%\n",
    "\n",
    "• GeLU coefficient (a):        {fits['gelu']['a']:.4e}\n",
    "• SwiGLU coefficient (a):      {fits['swiglu']['a']:.4e}\n",
    "• Coefficient change:          {abs(coeff_change):.1f}%\n",
    "\"\"\")\n",
    "    \n",
    "    if exp_diff < 5:\n",
    "        print(f\"\"\"CONCLUSION:\n",
    "SwiGLU improves the scaling coefficient (a) by {abs(coeff_change):.1f}% but leaves\n",
    "the exponent (b) essentially unchanged ({exp_diff:.1f}% difference).\n",
    "\n",
    "This suggests SwiGLU provides a CONSTANT COMPUTE MULTIPLIER ADVANTAGE\n",
    "rather than fundamentally altering how well the model scales.\"\"\")\n",
    "    else:\n",
    "        print(f\"\"\"CONCLUSION:\n",
    "SwiGLU shows a {exp_diff:.1f}% difference in scaling exponent compared to GeLU.\n",
    "This suggests SwiGLU may FUNDAMENTALLY ALTER the scaling behavior.\"\"\")\n",
    "else:\n",
    "    print(\"Insufficient data to draw conclusions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Download results (Colab only)\ntry:\n    from google.colab import files\n    files.download(\"scaling_plot.png\")\n    files.download(\"logs/all_results.json\")\nexcept ImportError:\n    print(\"Not running in Colab. Results saved to:\")\n    print(\"  - scaling_plot.png\")\n    print(\"  - logs/all_results.json\")"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}